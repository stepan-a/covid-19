{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 World Charts\n",
    "Guillaume Rozier, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nLICENSE MIT\\n2020\\nGuillaume Rozier\\nWebsite : http://www.guillaumerozier.fr\\nMail : guillaume.rozier@telecomnancy.net\\n\\nThis file contains scripts that download data from CSSE (John Hopkins) Github Repository and then process it to build many graphes.\\nI'm currently cleaning the code, please come back soon it will be easier to read and edit it!\\n\\nThe charts are exported to 'charts/images/'.\\nData is download to/imported from 'data/'.\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "LICENSE MIT\n",
    "2020\n",
    "Guillaume Rozier\n",
    "Website : http://www.guillaumerozier.fr\n",
    "Mail : guillaume.rozier@telecomnancy.net\n",
    "\n",
    "This file contains scripts that download data from CSSE (John Hopkins) Github Repository and then process it to build many graphes.\n",
    "I'm currently cleaning the code, please come back soon it will be easier to read and edit it!\n",
    "\n",
    "The charts are exported to 'charts/images/'.\n",
    "Data is download to/imported from 'data/'.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'build : 2020-05-21 22:26'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import sys\n",
    "import chart_studio\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly\n",
    "from plotly.subplots import make_subplots\n",
    "import chart_studio.plotly as py\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.validators.scatter.marker import SymbolValidator\n",
    "\n",
    "colors = px.colors.qualitative.D3 + plotly.colors.DEFAULT_PLOTLY_COLORS + px.colors.qualitative.Plotly + px.colors.qualitative.Dark24 + px.colors.qualitative.Alphabet\n",
    "\n",
    "#If you want to uplaod charts to your Plotly account (and switch \"upload\" to True just below):\n",
    "#chart_studio.tools.set_credentials_file(username='', api_key='')\n",
    "\n",
    "today = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "\"build : \" + today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to display charts here, please change \"show\" variable to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload = False\n",
    "show = False\n",
    "export = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'build : 2020-05-21 22:26'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "if len(sys.argv) >= 2:\n",
    "    if (sys.argv[1]).lower() == \"true\":\n",
    "        upload = True\n",
    "    \n",
    "if len(sys.argv) >= 3:\n",
    "    if (sys.argv[2]).lower() == \"true\":\n",
    "        show = True\n",
    "\n",
    "if len(sys.argv) >= 4:\n",
    "    if (sys.argv[3]).lower() == \"true\":\n",
    "        export = True\n",
    "    \n",
    "\"build : \" + today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_offset(df, col_of_reference, col_to_align, countries):\n",
    "        \n",
    "    diffs = []\n",
    "    for offset in range(len(df)-15):\n",
    "        \n",
    "        a = df[col_of_reference][1:].shift(offset, fill_value=0)/countries[col_of_reference][\"pop\"]\n",
    "        b = df[col_to_align][1:]/countries[col_to_align][\"pop\"]\n",
    "        \n",
    "        if len(a) > len(b):\n",
    "            a = a[:-2]\n",
    "        m = min(len(a), len(b))\n",
    "            \n",
    "        delta = ((a[offset:] - b[offset:])**2)**(1/2)\n",
    "        diffs.append(abs(delta.sum()))\n",
    "        xa = [i for i in range(offset, len(a))]\n",
    "        xb = [i for i in range(offset, len(b))]\n",
    "\n",
    "    ret = diffs.index(min(diffs))\n",
    "\n",
    "    if col_of_reference == col_to_align:\n",
    "        return 0\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    #url_confirmed = \"https://cowid.netlify.com/data/total_cases.csv\"\n",
    "    #url_deaths = \"https://cowid.netlify.com/data/total_deaths.csv\"\n",
    "    url_confirmed_csse = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\"    \n",
    "    url_deaths_csse = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv\"    \n",
    "    url_france_data = \"https://raw.githubusercontent.com/opencovid19-fr/data/master/dist/chiffres-cles.csv\"\n",
    "\n",
    "    #r_confirmed = requests.get(url_confirmed)\n",
    "    #r_deaths = requests.get(url_deaths)\n",
    "    r_confirmed_csse = requests.get(url_confirmed_csse)\n",
    "    r_deaths_csse = requests.get(url_deaths_csse)\n",
    "    r_france_data = requests.get(url_france_data)\n",
    "\n",
    "    #with open('data/total_cases_who.csv', 'wb') as f:\n",
    "        #f.write(r_confirmed.content)\n",
    "\n",
    "    #with open('data/total_deaths_who.csv', 'wb') as f:\n",
    "        #f.write(r_deaths.content)\n",
    "\n",
    "    with open('data/total_cases_csse.csv', 'wb') as f:\n",
    "            f.write(r_confirmed_csse.content)\n",
    "\n",
    "    with open('data/total_deaths_csse.csv', 'wb') as f:\n",
    "        f.write(r_deaths_csse.content)\n",
    "    \n",
    "    with open('data/france_data.csv', 'wb') as f:\n",
    "        f.write(r_france_data.content)\n",
    "\n",
    "    print(\"> data downloaded\")\n",
    "    #\"build : \" + today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_files(): \n",
    "    # CSSE data\n",
    "    df_confirmed_csse = pd.read_csv('data/total_cases_csse.csv')\n",
    "    df_deaths_csse = pd.read_csv('data/total_deaths_csse.csv')\n",
    "\n",
    "    # WHO data\n",
    "    #df_confirmed_who = pd.read_csv('data/total_cases_who.csv')\n",
    "    #df_deaths_who = pd.read_csv('data/total_deaths_who.csv')\n",
    "\n",
    "    # Perso data\n",
    "    df_confirmed_perso = pd.read_csv('data/total_cases_perso.csv')\n",
    "    df_deaths_perso = pd.read_csv('data/total_deaths_perso.csv')\n",
    "    df_france_data = pd.read_csv('data/france_data.csv')\n",
    "\n",
    "    print(\"> data imported\")\n",
    "    return df_confirmed_csse, df_deaths_csse, df_confirmed_perso, df_deaths_perso, df_france_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_csse(df0):\n",
    "    df = df0.drop('Lat', axis=1)\n",
    "    df = df.drop('Long', axis=1)\n",
    "    df = df.drop('Province/State', axis=1)\n",
    "    #df_csse_new2 = df_csse_new.groupby(['Country/Region'])\n",
    "    df = df.T.reset_index()\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.rename(columns={\"Country/Region\": \"date\"})\n",
    "    df = df.drop(df.index[0])\n",
    "    dates = df['date'].values\n",
    "    df = df.groupby(by=df.columns, axis=1).sum(numeric_only=True)\n",
    "    df['date'] = dates\n",
    "    return df\n",
    "\n",
    "#\"build : \" + today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_merge(data_confirmed, df_confirmed_perso, data_deaths, df_deaths_perso, df_france_data):\n",
    "    \"\"\"data_confirmed['date'] = data_confirmed['date'].astype('datetime64[ns]') \n",
    "    data_deaths['date'] = data_deaths['date'].astype('datetime64[ns]') \n",
    "\n",
    "    df_france_data = df_france_data[df_france_data['granularite']=='pays']\n",
    "    df_france_data_deaths = pd.DataFrame()\n",
    "    df_france_data_deaths['date'] = df_france_data['date']\n",
    "    df_france_data_deaths['France'] = df_france_data['deces']\n",
    "    df_france_data_deaths = df_france_data_deaths.dropna().drop_duplicates(subset=['date'], keep='last').reset_index()\n",
    "    df_france_data_deaths['date'] = df_france_data_deaths['date'].astype('datetime64[ns]') \n",
    "    #data_deaths = pd.merge(df_france_data_deaths, data_deaths, how='outer').drop_duplicates(subset=['date'])\n",
    "    #data_deaths = pd.merge(data_deaths[['date','France']], df_france_data_deaths, how='left') \n",
    "    data_deaths.set_index('date').join(df_france_data_deaths.set_index('date'))\n",
    "    \n",
    "    df_france_data_confirmed = pd.DataFrame()\n",
    "    df_france_data_confirmed['date'] = df_france_data['date']\n",
    "    df_france_data_confirmed['France'] = df_france_data['cas_confirmes']\n",
    "    df_france_data_confirmed = df_france_data_confirmed.dropna().drop_duplicates(subset=['date']).reset_index()\n",
    "    df_france_data_confirmed['date'] = df_france_data_confirmed['date'].astype('datetime64[ns]') \n",
    "    data_confirmed = pd.merge(data_confirmed, df_france_data_confirmed, how='outer').drop_duplicates(subset=['date'], keep='last')\n",
    "    \"\"\"\n",
    "    data_confirmed = pd.merge(data_confirmed, df_confirmed_perso, how='outer').drop_duplicates(subset='date')\n",
    "    data_deaths = pd.merge(data_deaths, df_deaths_perso, how='outer').drop_duplicates(subset='date')\n",
    "\n",
    "    #######\n",
    "    #date_int = [i for i in range(len(data_confirmed))]\n",
    "    #data_confirmed[\"date_int\"] = date_int\n",
    "\n",
    "    #date_int = [i for i in range(len(data_deaths))]\n",
    "    #data_deaths[\"date_int\"] = date_int\n",
    "\n",
    "    \"build : \" + today\n",
    "    #data_confirmed['date']\n",
    "    #df_deaths_perso.iloc[-1]\n",
    "    \n",
    "    #for c in countries:\n",
    "     #    data_deaths[c+\"_new\"] = data_deaths[c].diff()\n",
    "    return data_confirmed, data_deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rolling(df):\n",
    "    df_r = df\n",
    "    df_r[:len(df_r)-1].fillna(method='pad',inplace=True)\n",
    "    df_r = df.rolling(5, win_type='gaussian', center=True).mean(std=2)\n",
    "    df_r['date'] = df['date'].values\n",
    "    df_r.iloc[len(df_r)-2] = df.iloc[-2]\n",
    "    df_r.iloc[len(df_r)-1] = df.iloc[-1]\n",
    "\n",
    "    #moins_2 = ((df.iloc[-3][:-1] + df.iloc[-1][:-1]) / 2).append(pd.Series([df.iloc[-2][\"date\"]]))\n",
    "    #moins_1 = ((df.iloc[-3][:-1] + df.iloc[-1][:-1]) / 2).append(pd.Series([df.iloc[-1][\"date\"]]))\n",
    "\n",
    "    #df_r.iloc[-2] = moins_2\n",
    "    #df_r.iloc[-1] = moins_1\n",
    "    #data_confirmed.loc[:, data_confirmed.columns != \"date\"]\n",
    "    #df_r = df_r.drop(len(df_r)-1)\n",
    "    #df_r = df_r.drop(len(df_r)-1)\n",
    "    \n",
    "    df_r.loc[len(df_r)-3, df_r.columns != \"date\" ] = ((df.iloc[-4][:-1] + df.iloc[-2][:-1])/2 + df.iloc[-3][:-1])/2\n",
    "    df_r.loc[len(df_r)-3, \"date\"] = df.iloc[-3][\"date\"]\n",
    "    \n",
    "   # df_r.loc[len(df_r)-2, df_r.columns != \"date\" ] = ((df.iloc[-3][:-1] + df.iloc[-1][:-1])/2 + df.iloc[-2][:-1])/2\n",
    "    #df_r.loc[len(df_r)-2, \"date\"] = df.iloc[-2][\"date\"]\n",
    "    \n",
    "    df_r.loc[len(df_r)-2, df_r.columns != \"date\" ] = (df.iloc[-3][:-1] + (df.iloc[-3][:-1] - df.iloc[-4][:-1]) / 2 + df.iloc[-2][:-1])/2\n",
    "    df_r.loc[len(df_r)-2, \"date\"] = df.iloc[-2][\"date\"] \n",
    "    \n",
    "    df_r.loc[len(df_r)-1, df_r.columns != \"date\" ] = (df.iloc[-2][:-1] + (df.iloc[-1][:-1] - df.iloc[-3][:-1]) / 2 + df.iloc[-1][:-1])/2\n",
    "    df_r.loc[len(df_r)-1, \"date\"] = df.iloc[-1][\"date\"] \n",
    "    \n",
    "    return df_r\n",
    "\n",
    "\n",
    "def final_data_prep(data_confirmed, data_confirmed_rolling, data_deaths, data_deaths_rolling):\n",
    "    # Date conversion\n",
    "    data_confirmed['date'] = data_confirmed['date'].astype('datetime64[ns]') \n",
    "    #data_confirmed_rolling['date'] = data_confirmed_rolling['date'].astype('datetime64[ns]') \n",
    "\n",
    "    data_deaths['date'] = data_deaths['date'].astype('datetime64[ns]') \n",
    "    #data_deaths_rolling['date'] = data_deaths_rolling['date'].astype('datetime64[ns]') \n",
    "\n",
    "    date_int = [i for i in range(len(data_confirmed))]\n",
    "    data_confirmed[\"date_int\"] = date_int\n",
    "\n",
    "    date_int = [i for i in range(len(data_deaths))]\n",
    "    data_deaths[\"date_int\"] = date_int\n",
    "    \n",
    "    return data_confirmed, data_confirmed_rolling, data_deaths, data_deaths_rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data_confirmed_rolling.tail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Informations on countries (population, offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def offset_compute_export(data_confirmed, data_deaths):\n",
    "    # Importing informations on countries\n",
    "\n",
    "    with open('data/info_countries.json', 'r') as f:\n",
    "        countries = json.load(f)\n",
    "\n",
    "    # Computing offset\n",
    "    i = 0\n",
    "    for c in tqdm(countries):\n",
    "        countries[c]['offset_confirmed'] = compute_offset(data_confirmed, 'Italy', c, countries)\n",
    "        countries[c]['offset_deaths'] = compute_offset(data_deaths, 'Italy', c, countries)\n",
    "        countries[c]['color'] = i\n",
    "        i += 1\n",
    "    # Exporting informations on countries\n",
    "    with open('data/info_countries.json', 'w') as fp:\n",
    "        json.dump(countries, fp)\n",
    "\n",
    "    print(\"> pop data imported\")\n",
    "    \"build : \" + today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_df_exports(data_confirmed, data_deaths):\n",
    "    data_confirmed.to_csv('data/data_confirmed.csv')\n",
    "    data_deaths.to_csv('data/data_deaths.csv')\n",
    "    print(\"> dfs exported\")\n",
    "    \n",
    "def data_import():\n",
    "    with open('data/info_countries.json', 'r') as f:\n",
    "        countries = json.load(f)\n",
    "    return pd.read_csv('data/data_confirmed.csv'), pd.read_csv('data/data_deaths.csv'), countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data():\n",
    "    # Data update:\n",
    "    download_data()\n",
    "    df_confirmed_csse, df_deaths_csse, df_confirmed_perso, df_deaths_perso, df_france_data = import_files()\n",
    "\n",
    "    df_confirmed_csse = data_prep_csse(df_confirmed_csse)\n",
    "    df_deaths_csse = data_prep_csse(df_deaths_csse)\n",
    "\n",
    "    data_confirmed, data_deaths = data_merge(df_confirmed_csse, df_confirmed_perso, df_deaths_csse, df_deaths_perso, df_france_data)\n",
    "\n",
    "    #data_confirmed_rolling = rolling(data_confirmed)\n",
    "    #data_deaths_rolling = rolling(data_deaths)\n",
    "\n",
    "    data_confirmed, data_confirmed_rolling, data_deaths, data_deaths_rolling = final_data_prep(data_confirmed, \"data_confirmed_rolling\", data_deaths, \"data_deaths_rolling\")\n",
    "    \n",
    "    offset_compute_export(data_confirmed, data_deaths)\n",
    "\n",
    "    final_df_exports(data_confirmed, data_deaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df_confirmed_csse, df_deaths_csse, df_confirmed_perso, df_deaths_perso, df_france_data = import_files()\\n\\ndf_confirmed_csse = data_prep_csse(df_confirmed_csse)\\ndf_deaths_csse = data_prep_csse(df_deaths_csse)\\ndf_confirmed_csse['date'] = df_confirmed_csse['date'].astype('datetime64[ns]') \\n\\ndf_france_data = df_france_data[df_france_data['granularite']=='pays']\\n\\n\\n\\ndf_france_data_deaths = pd.DataFrame()\\ndf_france_data_deaths['date'] = df_france_data['date']\\ndf_france_data_deaths['France'] = df_france_data['deces']\\ndf_france_data_deaths = df_france_data_deaths.dropna().drop_duplicates(subset=['date']).reset_index()\\n#df_france_data_deaths['date'] = df_france_data_deaths['date'].astype('datetime64[ns]') \\ndata_confirmed = pd.merge(data_deaths_csse, df_france_data_deaths, how='outer')\\n\\n\\n\\ndf_france_data_confirmed = pd.DataFrame()\\ndf_france_data_confirmed['date'] = df_france_data['date']\\ndf_france_data_confirmed['France'] = df_france_data['cas_confirmes']\\ndf_france_data_confirmed = df_france_data_confirmed.dropna().drop_duplicates(subset=['date']).reset_index()\\ndf_france_data_confirmed['date'] = df_france_data_confirmed['date'].astype('datetime64[ns]') \\ndata_confirmed = pd.merge(df_confirmed_csse, df_france_data_confirmed, how='outer')\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"df_confirmed_csse, df_deaths_csse, df_confirmed_perso, df_deaths_perso, df_france_data = import_files()\n",
    "\n",
    "df_confirmed_csse = data_prep_csse(df_confirmed_csse)\n",
    "df_deaths_csse = data_prep_csse(df_deaths_csse)\n",
    "df_confirmed_csse['date'] = df_confirmed_csse['date'].astype('datetime64[ns]') \n",
    "\n",
    "df_france_data = df_france_data[df_france_data['granularite']=='pays']\n",
    "\n",
    "\n",
    "\n",
    "df_france_data_deaths = pd.DataFrame()\n",
    "df_france_data_deaths['date'] = df_france_data['date']\n",
    "df_france_data_deaths['France'] = df_france_data['deces']\n",
    "df_france_data_deaths = df_france_data_deaths.dropna().drop_duplicates(subset=['date']).reset_index()\n",
    "#df_france_data_deaths['date'] = df_france_data_deaths['date'].astype('datetime64[ns]') \n",
    "data_confirmed = pd.merge(data_deaths_csse, df_france_data_deaths, how='outer')\n",
    "\n",
    "\n",
    "\n",
    "df_france_data_confirmed = pd.DataFrame()\n",
    "df_france_data_confirmed['date'] = df_france_data['date']\n",
    "df_france_data_confirmed['France'] = df_france_data['cas_confirmes']\n",
    "df_france_data_confirmed = df_france_data_confirmed.dropna().drop_duplicates(subset=['date']).reset_index()\n",
    "df_france_data_confirmed['date'] = df_france_data_confirmed['date'].astype('datetime64[ns]') \n",
    "data_confirmed = pd.merge(df_confirmed_csse, df_france_data_confirmed, how='outer')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'download_data()\\ndf_confirmed_csse, df_deaths_csse, df_confirmed_perso, df_deaths_perso, df_france_data = import_files()\\n\\ndf_confirmed_csse = data_prep_csse(df_confirmed_csse)\\ndf_deaths_csse = data_prep_csse(df_deaths_csse)\\n\\n#data_confirmed[\\'date\\'] = data_confirmed[\\'date\\'].astype(\\'datetime64[ns]\\') \\ndata_deaths[\\'date\\'] = data_deaths[\\'date\\'].astype(\\'datetime64[ns]\\') \\n\\n\\ndf_france_data_confirmed = pd.DataFrame()\\ndf_france_data_confirmed[\\'date\\'] = df_france_data[\\'date\\']\\ndf_france_data_confirmed[\\'France_corr\\'] = df_france_data[\\'cas_confirmes\\']\\ndf_france_data_confirmed = df_france_data_confirmed.dropna().drop_duplicates(subset=[\\'date\\']).reset_index()\\n#df_france_data_confirmed[\\'date\\'] = df_france_data_confirmed[\\'date\\'].astype(\\'datetime64[ns]\\') \\ndata_confirmed = pd.merge(data_confirmed, df_france_data_confirmed, how=\\'outer\\').drop_duplicates(subset=[\\'date\\'], keep=\\'last\\')\\ndata_confirmed.join(df_france_data_confirmed.set_index(\\'date\\'), lsuffix=\\'_caller\\', rsuffix=\\'_other\\', on=\"date\")'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"download_data()\n",
    "df_confirmed_csse, df_deaths_csse, df_confirmed_perso, df_deaths_perso, df_france_data = import_files()\n",
    "\n",
    "df_confirmed_csse = data_prep_csse(df_confirmed_csse)\n",
    "df_deaths_csse = data_prep_csse(df_deaths_csse)\n",
    "\n",
    "#data_confirmed['date'] = data_confirmed['date'].astype('datetime64[ns]') \n",
    "data_deaths['date'] = data_deaths['date'].astype('datetime64[ns]') \n",
    "\n",
    "\n",
    "df_france_data_confirmed = pd.DataFrame()\n",
    "df_france_data_confirmed['date'] = df_france_data['date']\n",
    "df_france_data_confirmed['France_corr'] = df_france_data['cas_confirmes']\n",
    "df_france_data_confirmed = df_france_data_confirmed.dropna().drop_duplicates(subset=['date']).reset_index()\n",
    "#df_france_data_confirmed['date'] = df_france_data_confirmed['date'].astype('datetime64[ns]') \n",
    "data_confirmed = pd.merge(data_confirmed, df_france_data_confirmed, how='outer').drop_duplicates(subset=['date'], keep='last')\n",
    "data_confirmed.join(df_france_data_confirmed.set_index('date'), lsuffix='_caller', rsuffix='_other', on=\"date\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function\n",
    "This fonction builds and export graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chart(data, data_rolling, countries, by_million_inh = False, align_curves = False, last_d = 15, offset_name = 'offset_confirmed', type_ppl = \"confirmed cases\", name_fig=\"\", since=False, min_rate=0, log=False, new=\"\"):\n",
    "    today = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "    ### Symbols\n",
    "    symbols = []\n",
    "    for i in range(35):\n",
    "        symbols.append(SymbolValidator().values[i])\n",
    "    random.shuffle(symbols)\n",
    "    ###\n",
    "    \n",
    "    fig = go.Figure()\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    x_an=np.array([])\n",
    "    y_an=np.array([])\n",
    "    \n",
    "    countries_last_val = []\n",
    "    countries_array = []\n",
    "    for c in countries:\n",
    "        if by_million_inh:\n",
    "             val = data[c][len(data) - 1]/countries[c]['pop']\n",
    "        else:\n",
    "            val = data[c][len(data) - 1]\n",
    "            \n",
    "        countries_last_val.append(val)\n",
    "        countries_array.append(c)\n",
    "        \n",
    "    ind = np.argsort(countries_last_val)\n",
    "    countries_array = np.array(countries_array)\n",
    "    countries_array = countries_array[ind][::-1]\n",
    "        \n",
    "    for c in countries_array:\n",
    "\n",
    "        if align_curves:\n",
    "            offset = countries[c][offset_name]\n",
    "            offset2 = -offset\n",
    "        else:\n",
    "            offset = 0\n",
    "\n",
    "        if offset==0: offset2 = None\n",
    "\n",
    "        if by_million_inh:\n",
    "            pop = countries[c]['pop']\n",
    "        else:\n",
    "            pop = 1\n",
    "        \n",
    "        date = 'date'\n",
    "        offset3=0\n",
    "        since_str = \"\"\n",
    "        since_str_leg = \"\"\n",
    "        \n",
    "        if since:\n",
    "            date = 'date_int'\n",
    "            res = list(map(lambda i: i> min_rate, data[c+new].values / pop))\n",
    "            offset2 = 0\n",
    "            if True in res:\n",
    "                ind = res.index(True) \n",
    "                offset2 = -ind\n",
    "                since_str_leg = \" [since {} days]\".format(len(data) - ind)\n",
    "\n",
    "            offset3 = offset2\n",
    "            last_d = 0\n",
    "            offset = 0\n",
    "            since_str = \" [since {}]\".format(min_rate) #, type_ppl\n",
    "            \n",
    "            if by_million_inh:\n",
    "                since_str = since_str[:-1] + \"/1M inh.]\"\n",
    "                \n",
    "\n",
    "        x = data[date][ -last_d - offset: offset2]\n",
    "        y = data[c+new][-last_d - offset3:] / pop\n",
    "        \n",
    "        if offset != 0:\n",
    "            name_legend = '{} [delayed by {} days]'.format(c, -offset)\n",
    "        else:\n",
    "            name_legend = '{} {}'.format(c, since_str_leg)\n",
    "        txt=[\"\" for i in range(len(data_rolling[c][-last_d - offset3:]))]\n",
    "        txt[-1] = c\n",
    "        fig.add_trace(go.Scatter(x = x, y = y,\n",
    "                        mode='markers',\n",
    "                        marker_color = colors[countries[c]['color']],\n",
    "                        legendgroup = c,\n",
    "                        marker_symbol = countries[c]['color'],\n",
    "                        marker_size=9,\n",
    "                        #marker_line_width=2,\n",
    "                        opacity=1,\n",
    "                        showlegend=True,\n",
    "                        name = name_legend))\n",
    "        \n",
    "        fig.add_trace(go.Scatter(x = data_rolling[date][ -last_d - offset : offset2], y = data_rolling[c+new][-last_d - offset3:] / pop,\n",
    "                        mode='lines',\n",
    "                        marker_color = colors[countries[c]['color']],\n",
    "                        opacity = 1,\n",
    "                        legendgroup=c,\n",
    "                        showlegend=False,\n",
    "                        line=dict(width=2),\n",
    "                        name = name_legend))\n",
    "        i += 1\n",
    "        j += 1\n",
    "        \n",
    "        if i >= len(colors):\n",
    "            i = 0\n",
    "            \n",
    "        if j >= 40:\n",
    "            j = 0\n",
    "        \n",
    "        if log and since and c==\"Italy\":\n",
    "            date_start = data_rolling['date_int'].values[ -last_d - offset]\n",
    "            \n",
    "            x = data_rolling[\"date_int\"][ -last_d - offset : offset2]\n",
    "            \n",
    "            max_values = 15\n",
    "            for (rate, rate_str) in [(2**(1/10), \"x2 every 10 days\"), (2**(1/7), \"x2 every 7 days\"), (2**(1/3), \"x2 every 3 days\"), (2**(1/2), \"x2 every 2 days\"), (2**(1/5), \"x2 every 5 days\")]:\n",
    "                \n",
    "                y = rate ** (data_rolling[\"date_int\"][ -last_d - offset : offset2].values - date_start) * min_rate\n",
    "                \n",
    "                fig.add_trace(go.Scatter(x = x[:max_values+1], y = y[:max_values+1],\n",
    "                                mode='lines+text',\n",
    "                                marker_color=\"grey\",\n",
    "                                opacity=1,\n",
    "                                #text = rate_str,\n",
    "                                textposition = \"bottom right\",\n",
    "                                legendgroup=\"Tendance\",\n",
    "                                showlegend=False,\n",
    "                                line=dict(\n",
    "                                    width=1,\n",
    "                                    dash='dot'\n",
    "                                ),\n",
    "                                name = \"Tendance\"))\n",
    "\n",
    "                fig.add_trace(go.Scatter(x = [data_rolling[\"date_int\"][ -last_d - offset : offset2].values[max_values]], y = [(rate ** (data_rolling[\"date_int\"][ -last_d - offset : offset2].values - date_start) * min_rate)[max_values]],\n",
    "                                mode='text',\n",
    "                                marker_color=\"grey\",\n",
    "                                opacity=1,\n",
    "                                text = rate_str,\n",
    "                                textposition = \"bottom right\",\n",
    "                                legendgroup=\"Tendance\",\n",
    "                                showlegend=False,\n",
    "                                name = \"Tendance\"))\n",
    "            \n",
    "    ### END LOOP ###\n",
    "    \n",
    "    align_str = \"\"\n",
    "    if align_curves:\n",
    "        align_str = \" [aligned]\"\n",
    "        \n",
    "    million_str = \"\"\n",
    "    million_str_ax = \"\"\n",
    "    if by_million_inh:\n",
    "        million_str = \" for 1M inhabitants\"\n",
    "        million_str_ax = \"/ nb of inhabitants (million)\"\n",
    "        \n",
    "    delayed=\"\"\n",
    "    if align_curves:\n",
    "        delayed=\"— delayed for some countries\"\n",
    "    if since:\n",
    "        delayed =\"— since {} {} {}\".format(min_rate, type_ppl, million_str)\n",
    "    \n",
    "    fig.update_annotations(dict(\n",
    "            xref=\"x\",\n",
    "            yref=\"y\",\n",
    "            showarrow=True,\n",
    "            arrowhead=7\n",
    "    ))\n",
    "    log_str=\"linear\"\n",
    "    \n",
    "    if log:\n",
    "        log_str = \"log\"\n",
    "    \n",
    "    fig.update_layout(\n",
    "        showlegend=True,\n",
    "        title={\n",
    "            'text': \"COVID-19 <b>{}{}</b>{}{}\".format(type_ppl, million_str, align_str, since_str),\n",
    "            'y':0.95,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'},\n",
    "        xaxis_title=\"Day {} {}\".format(delayed, ''),\n",
    "        yaxis_type=log_str,\n",
    "        yaxis_title=\"Total {} {}\".format(type_ppl, million_str),\n",
    "        titlefont = dict(\n",
    "            size=28),\n",
    "        annotations = [dict(xref='paper',\n",
    "            yref='paper',\n",
    "            x=0, y=1.05,\n",
    "            showarrow=False,\n",
    "            text ='Last update: {} ; Last data: {} ; Data: CSSE ; Author: @guillaumerozier'.format(today, str(data['date'].values[-1])[:10]))]\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(nticks = last_d)\n",
    "\n",
    "    print(\"> graph built\")\n",
    "\n",
    "    if upload:\n",
    "        py.plot(fig, filename = name_fig, auto_open=False)\n",
    "        print(\"> graph uploaded\")\n",
    "\n",
    "    if show:\n",
    "        fig.show()\n",
    "        print(\"> graph showed\")\n",
    "\n",
    "    if export:\n",
    "        path_log = \"\"\n",
    "        if log:\n",
    "            path_log = \"log_yaxis/\"\n",
    "        fig.write_image(\"images/charts/{}{}.png\".format(path_log, name_fig), scale=3, width=1100, height=700)\n",
    "        fig.write_image(\"images/charts_sd/{}{}.png\".format(path_log, name_fig), scale=0.5)\n",
    "        plotly.offline.plot(fig, filename = 'images/html_exports/{}{}.html'.format(path_log, name_fig), auto_open=False)\n",
    "        print(\"> graph exported\\n\")\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> data downloaded\n",
      "> data imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:07<00:00,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> pop data imported\n",
      "> dfs exported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "update_data()\n",
    "data_confirmed, data_deaths, countries = data_import()\n",
    "data_confirmed_t = data_confirmed.T\n",
    "data_confirmed_t.columns = data_confirmed_t.iloc[len(data_confirmed_t)-2]\n",
    "data_confirmed_t = data_confirmed_t.drop(data_confirmed_t.index[-1])\n",
    "data_confirmed_t = data_confirmed_t.drop(data_confirmed_t.index[-1])\n",
    "data_confirmed_t = data_confirmed_t.drop(data_confirmed_t.index[0])\n",
    "\n",
    "data_deaths_t = data_deaths.T\n",
    "data_deaths_t.columns = data_deaths_t.iloc[len(data_deaths_t)-2]\n",
    "data_deaths_t = data_deaths_t.drop(data_deaths_t.index[-1])\n",
    "data_deaths_t = data_deaths_t.drop(data_deaths_t.index[-1])\n",
    "data_deaths_t = data_deaths_t.drop(data_deaths_t.index[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> subplots_deaths_samescale\n",
      "> subplots_deaths\n",
      "> subplots_confirmed_samescale\n",
      "> subplots_confirmed\n"
     ]
    }
   ],
   "source": [
    "for (data, name_var, same_scale) in [(data_deaths_t, \"deaths\", True), (data_deaths_t, \"deaths\", False), (data_confirmed_t, \"confirmed\", True), (data_confirmed_t, \"confirmed\", False)]: \n",
    "    name_suffix=\"confirmed\"\n",
    "    type_ppl = \"cas positifs\"\n",
    "\n",
    "    if \"death\" in name_var:\n",
    "        name_suffix=\"deaths\"\n",
    "        type_ppl = \"décès\"\n",
    "\n",
    "    ni, nj = 4, 5\n",
    "    i, j = 1, 1\n",
    "\n",
    "    dates = data.columns.values\n",
    "\n",
    "    data = data.sort_values(by=[dates[-1]], ascending=False)\n",
    "    data = data.diff(axis=1).rolling(axis=1, window=7).mean()\n",
    "\n",
    "    countries_ordered = list(data.index.values[:20])\n",
    "    #countries_ordered[:11] + [\"\"] + countries_ordered[11:14] + [\"\"] + countries_ordered[14:]\n",
    "    max_value = 0\n",
    "\n",
    "    fig = make_subplots(rows=ni, cols=nj, shared_yaxes = same_scale, subplot_titles = [\"<b>\" + c + \"</b>\" for c in countries_ordered], vertical_spacing = 0.06, horizontal_spacing = 0.02)\n",
    "\n",
    "    sub = \"<sub>par ordre décroissant du cumul total, les croix représentent les données quotidiennes brutes et les bâtons la moyenne mobile sur 7 jours  •  guillaumerozier.fr</sub>\"\n",
    "\n",
    "    max_value_diff = 0\n",
    "\n",
    "    for country in countries_ordered:\n",
    "\n",
    "        data_c = data.loc[country].rolling(window = 7, center=True).mean()\n",
    "        fig.add_trace(go.Bar(x = data.loc[country].index, y = data_c,\n",
    "                            marker=dict(color = data_c.diff(), coloraxis=\"coloraxis\"), ),\n",
    "                      i, j)\n",
    "        fig.add_trace(go.Scatter(x = data.loc[country].index, y = data.loc[country],\n",
    "                    mode=\"markers\",\n",
    "                    marker_size=6,\n",
    "                    marker_symbol=\"x-thin\",\n",
    "                    marker_line_color=\"Black\", marker_line_width=0.6, opacity=0.5),\n",
    "                     i, j)\n",
    "\n",
    "        max_value = max(max_value, data_c.max())\n",
    "        max_value_diff = max(max_value_diff, data_c.diff().max())\n",
    "\n",
    "        rangemin = \"2020-02-02\"\n",
    "\n",
    "        fig.update_xaxes(title_text=\"\", range=[rangemin, dates[-1]], gridcolor='white', ticks=\"inside\", tickformat='%d/%m', tickangle=0, nticks=10, linewidth=1, linecolor='white', row=i, col=j)\n",
    "\n",
    "        rge = None\n",
    "        if same_scale:\n",
    "            rge = [0, max_value]\n",
    "        fig.update_yaxes(title_text=\"\", range=rge, gridcolor='white', linewidth=1, linecolor='white', row=i, col=j)\n",
    "\n",
    "        j+=1\n",
    "        if j == nj+1 : #or ((i >= 3) & (j == nj))\n",
    "            i+=1\n",
    "            j=1\n",
    "\n",
    "\n",
    "    for i in fig['layout']['annotations']:\n",
    "        i['font'] = dict(size=25)\n",
    "        i['y'] = i['y'] - 0.04\n",
    "\n",
    "    #for annotation in fig['layout']['annotations']: \n",
    "            #annotation ['x'] = 0.5\n",
    "    by_million_title = \"\"\n",
    "    by_million_legend = \"\"\n",
    "\n",
    "    fig.update_layout(\n",
    "        barmode=\"overlay\",\n",
    "        margin=dict(\n",
    "            l=0,\n",
    "            r=25,\n",
    "            b=0,\n",
    "            t=160,\n",
    "            pad=0\n",
    "        ),\n",
    "        bargap=0,\n",
    "        paper_bgcolor='#fffdf5',#fcf8ed #faf9ed\n",
    "        plot_bgcolor='#f5f0e4',#f5f0e4 fcf8ed f0e8d5 \n",
    "        coloraxis=dict(colorscale=[\"green\", \"#ffc832\", \"#cf0000\"], cmin=-max_value_diff/4, cmax=max_value_diff/4), \n",
    "                    coloraxis_colorbar=dict(\n",
    "                        title=\"Nombre<br>quotidien<br>de {}<br>&#8205;<br>&#8205; \".format(type_ppl),\n",
    "                        thicknessmode=\"pixels\", thickness=15,\n",
    "                        lenmode=\"pixels\", len=600,\n",
    "                        yanchor=\"middle\", y=0.5, xanchor=\"left\", x=1.02,\n",
    "                        ticks=\"outside\", tickprefix=\"  \", ticksuffix=\"\",\n",
    "                        nticks=15,\n",
    "                        tickfont=dict(size=15),\n",
    "                        titlefont=dict(size=18)),\n",
    "\n",
    "                    showlegend=False,\n",
    "\n",
    "                     title={\n",
    "                        'text': (\"COVID19 : <b>nombre de {} quotidiens</b><br>\"+sub).format(type_ppl),\n",
    "                        'y':0.97,\n",
    "                        'x':0.5,\n",
    "                        'xref':\"paper\",\n",
    "                         'yref':\"container\",\n",
    "                        'xanchor': 'center',\n",
    "                        'yanchor': 'middle'},\n",
    "                        titlefont = dict(\n",
    "                        size=45,\n",
    "                        )\n",
    "    )\n",
    "\n",
    "    fig[\"layout\"][\"annotations\"] += ( dict(\n",
    "                            x=0.9,\n",
    "                            y=0.015,\n",
    "                            xref='paper',\n",
    "                            yref='paper',\n",
    "                            xanchor='center',\n",
    "                            yanchor='top',\n",
    "                            text='Source :<br>Santé Publique France',\n",
    "                            showarrow = False,\n",
    "                            font=dict(size=12), \n",
    "                            opacity=0.5\n",
    "                        ),)\n",
    "\n",
    "    if same_scale:\n",
    "        same_scale_str = \"_samescale\"\n",
    "    else:\n",
    "        same_scale_str = \"\"\n",
    "\n",
    "    name_fig = \"subplots_\" + name_suffix + same_scale_str\n",
    "    fig.write_image(\"images/charts/{}.png\".format(name_fig), scale=2, width=3000, height=1650)\n",
    "\n",
    "    fig[\"layout\"][\"annotations\"] += (\n",
    "                    dict(\n",
    "                        x=0.5,\n",
    "                        y=1,\n",
    "                        xref='paper',\n",
    "                        yref='paper',\n",
    "                        xanchor='center',\n",
    "                        text='Cliquez sur des éléments de légende pour les ajouter/supprimer',\n",
    "                        showarrow = False\n",
    "                        ),\n",
    "                        )\n",
    "    plotly.offline.plot(fig, filename = 'images/html_exports/{}.html'.format(name_fig), auto_open=False)\n",
    "    print(\"> \" + name_fig)\n",
    "\n",
    "\n",
    "    #fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>date</th>\n",
       "      <th>2020-01-22</th>\n",
       "      <th>2020-01-23</th>\n",
       "      <th>2020-01-24</th>\n",
       "      <th>2020-01-25</th>\n",
       "      <th>2020-01-26</th>\n",
       "      <th>2020-01-27</th>\n",
       "      <th>2020-01-28</th>\n",
       "      <th>2020-01-29</th>\n",
       "      <th>2020-01-30</th>\n",
       "      <th>2020-01-31</th>\n",
       "      <th>...</th>\n",
       "      <th>2020-05-11</th>\n",
       "      <th>2020-05-12</th>\n",
       "      <th>2020-05-13</th>\n",
       "      <th>2020-05-14</th>\n",
       "      <th>2020-05-15</th>\n",
       "      <th>2020-05-16</th>\n",
       "      <th>2020-05-17</th>\n",
       "      <th>2020-05-18</th>\n",
       "      <th>2020-05-19</th>\n",
       "      <th>2020-05-20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.035102e+02</td>\n",
       "      <td>-5.457959e+02</td>\n",
       "      <td>-5.785102e+02</td>\n",
       "      <td>-5.479184e+02</td>\n",
       "      <td>-4.402653e+02</td>\n",
       "      <td>-3.824694e+02</td>\n",
       "      <td>-2.800612e+02</td>\n",
       "      <td>-1.444694e+02</td>\n",
       "      <td>-1.190408e+02</td>\n",
       "      <td>7.591837e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russia</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.664286e+02</td>\n",
       "      <td>3.073673e+02</td>\n",
       "      <td>2.002449e+02</td>\n",
       "      <td>9.026531e+01</td>\n",
       "      <td>3.175510e+01</td>\n",
       "      <td>-2.561224e+01</td>\n",
       "      <td>-5.993878e+01</td>\n",
       "      <td>-1.375918e+02</td>\n",
       "      <td>-1.872449e+02</td>\n",
       "      <td>-2.022041e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brazil</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.040816e+02</td>\n",
       "      <td>4.191633e+02</td>\n",
       "      <td>3.387755e+02</td>\n",
       "      <td>3.837959e+02</td>\n",
       "      <td>3.817347e+02</td>\n",
       "      <td>3.773265e+02</td>\n",
       "      <td>3.573061e+02</td>\n",
       "      <td>5.061224e+02</td>\n",
       "      <td>6.308571e+02</td>\n",
       "      <td>7.737959e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United Kingdom</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.008163e+01</td>\n",
       "      <td>-4.887755e+01</td>\n",
       "      <td>-1.488163e+02</td>\n",
       "      <td>-1.843469e+02</td>\n",
       "      <td>-1.748776e+02</td>\n",
       "      <td>-1.650816e+02</td>\n",
       "      <td>-1.645102e+02</td>\n",
       "      <td>-1.861837e+02</td>\n",
       "      <td>-1.857347e+02</td>\n",
       "      <td>-2.039184e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spain</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.793878e+01</td>\n",
       "      <td>2.959184e+00</td>\n",
       "      <td>1.955102e+01</td>\n",
       "      <td>1.653061e+00</td>\n",
       "      <td>-6.428571e+00</td>\n",
       "      <td>2.530612e+00</td>\n",
       "      <td>-1.093878e+01</td>\n",
       "      <td>-1.072449e+02</td>\n",
       "      <td>-9.579592e+01</td>\n",
       "      <td>-9.187755e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seychelles</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.379049e-17</td>\n",
       "      <td>2.379049e-17</td>\n",
       "      <td>2.379049e-17</td>\n",
       "      <td>2.379049e-17</td>\n",
       "      <td>2.379049e-17</td>\n",
       "      <td>2.379049e-17</td>\n",
       "      <td>2.379049e-17</td>\n",
       "      <td>2.379049e-17</td>\n",
       "      <td>2.379049e-17</td>\n",
       "      <td>2.379049e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MS Zaandam</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Papua New Guinea</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Western Sahara</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lesotho</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.040816e-02</td>\n",
       "      <td>2.040816e-02</td>\n",
       "      <td>2.040816e-02</td>\n",
       "      <td>2.040816e-02</td>\n",
       "      <td>2.040816e-02</td>\n",
       "      <td>2.040816e-02</td>\n",
       "      <td>2.040816e-02</td>\n",
       "      <td>-2.040816e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "date              2020-01-22  2020-01-23  2020-01-24  2020-01-25  2020-01-26  \\\n",
       "US                       NaN         NaN         NaN         NaN         NaN   \n",
       "Russia                   NaN         NaN         NaN         NaN         NaN   \n",
       "Brazil                   NaN         NaN         NaN         NaN         NaN   \n",
       "United Kingdom           NaN         NaN         NaN         NaN         NaN   \n",
       "Spain                    NaN         NaN         NaN         NaN         NaN   \n",
       "...                      ...         ...         ...         ...         ...   \n",
       "Seychelles               NaN         NaN         NaN         NaN         NaN   \n",
       "MS Zaandam               NaN         NaN         NaN         NaN         NaN   \n",
       "Papua New Guinea         NaN         NaN         NaN         NaN         NaN   \n",
       "Western Sahara           NaN         NaN         NaN         NaN         NaN   \n",
       "Lesotho                  NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "date              2020-01-27  2020-01-28  2020-01-29  2020-01-30  2020-01-31  \\\n",
       "US                       NaN         NaN         NaN         NaN         NaN   \n",
       "Russia                   NaN         NaN         NaN         NaN         NaN   \n",
       "Brazil                   NaN         NaN         NaN         NaN         NaN   \n",
       "United Kingdom           NaN         NaN         NaN         NaN         NaN   \n",
       "Spain                    NaN         NaN         NaN         NaN         NaN   \n",
       "...                      ...         ...         ...         ...         ...   \n",
       "Seychelles               NaN         NaN         NaN         NaN         NaN   \n",
       "MS Zaandam               NaN         NaN         NaN         NaN         NaN   \n",
       "Papua New Guinea         NaN         NaN         NaN         NaN         NaN   \n",
       "Western Sahara           NaN         NaN         NaN         NaN         NaN   \n",
       "Lesotho                  NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "date              ...    2020-05-11    2020-05-12    2020-05-13    2020-05-14  \\\n",
       "US                ... -5.035102e+02 -5.457959e+02 -5.785102e+02 -5.479184e+02   \n",
       "Russia            ...  3.664286e+02  3.073673e+02  2.002449e+02  9.026531e+01   \n",
       "Brazil            ...  4.040816e+02  4.191633e+02  3.387755e+02  3.837959e+02   \n",
       "United Kingdom    ... -2.008163e+01 -4.887755e+01 -1.488163e+02 -1.843469e+02   \n",
       "Spain             ...  1.793878e+01  2.959184e+00  1.955102e+01  1.653061e+00   \n",
       "...               ...           ...           ...           ...           ...   \n",
       "Seychelles        ...  2.379049e-17  2.379049e-17  2.379049e-17  2.379049e-17   \n",
       "MS Zaandam        ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "Papua New Guinea  ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "Western Sahara    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "Lesotho           ...  0.000000e+00  0.000000e+00  2.040816e-02  2.040816e-02   \n",
       "\n",
       "date                2020-05-15    2020-05-16    2020-05-17    2020-05-18  \\\n",
       "US               -4.402653e+02 -3.824694e+02 -2.800612e+02 -1.444694e+02   \n",
       "Russia            3.175510e+01 -2.561224e+01 -5.993878e+01 -1.375918e+02   \n",
       "Brazil            3.817347e+02  3.773265e+02  3.573061e+02  5.061224e+02   \n",
       "United Kingdom   -1.748776e+02 -1.650816e+02 -1.645102e+02 -1.861837e+02   \n",
       "Spain            -6.428571e+00  2.530612e+00 -1.093878e+01 -1.072449e+02   \n",
       "...                        ...           ...           ...           ...   \n",
       "Seychelles        2.379049e-17  2.379049e-17  2.379049e-17  2.379049e-17   \n",
       "MS Zaandam        0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "Papua New Guinea  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "Western Sahara    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "Lesotho           2.040816e-02  2.040816e-02  2.040816e-02  2.040816e-02   \n",
       "\n",
       "date                2020-05-19    2020-05-20  \n",
       "US               -1.190408e+02  7.591837e+00  \n",
       "Russia           -1.872449e+02 -2.022041e+02  \n",
       "Brazil            6.308571e+02  7.737959e+02  \n",
       "United Kingdom   -1.857347e+02 -2.039184e+02  \n",
       "Spain            -9.579592e+01 -9.187755e+01  \n",
       "...                        ...           ...  \n",
       "Seychelles        2.379049e-17  2.379049e-17  \n",
       "MS Zaandam        0.000000e+00  0.000000e+00  \n",
       "Papua New Guinea  0.000000e+00  0.000000e+00  \n",
       "Western Sahara    0.000000e+00  0.000000e+00  \n",
       "Lesotho           2.040816e-02 -2.040816e-02  \n",
       "\n",
       "[188 rows x 120 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.diff(axis=1).rolling(axis=1, window=7).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function calls\n",
    "This block contains calls to above function for every chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> data downloaded\n",
      "> data imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:06<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> pop data imported\n",
      "> dfs exported\n",
      "cases\n",
      "> graph built\n",
      "> graph exported\n",
      "\n",
      "cases_per_1m_inhabitant\n",
      "> graph built\n",
      "> graph exported\n",
      "\n",
      "cases_per_1m_inhabitant_since\n",
      "> graph built\n",
      "> graph exported\n",
      "\n",
      "cases_since\n",
      "> graph built\n",
      "> graph exported\n",
      "\n",
      "deaths\n",
      "> graph built\n",
      "> graph exported\n",
      "\n",
      "deaths_per_1m_inhabitant\n",
      "> graph built\n",
      "> graph exported\n",
      "\n",
      "deaths_per_1m_inhabitant_since\n",
      "> graph built\n",
      "> graph exported\n",
      "\n",
      "deaths_since\n",
      "> graph built\n",
      "> graph exported\n",
      "\n",
      "cases\n",
      "> graph built\n",
      "> graph exported\n",
      "\n",
      "cases_per_1m_inhabitant\n",
      "> graph built\n",
      "> graph exported\n",
      "\n",
      "cases_per_1m_inhabitant_since\n",
      "> graph built\n",
      "> graph exported\n",
      "\n",
      "cases_since\n",
      "> graph built\n",
      "> graph exported\n",
      "\n",
      "deaths\n",
      "> graph built\n",
      "> graph exported\n",
      "\n",
      "deaths_per_1m_inhabitant\n",
      "> graph built\n",
      "> graph exported\n",
      "\n",
      "deaths_per_1m_inhabitant_since\n",
      "> graph built\n",
      "> graph exported\n",
      "\n",
      "deaths_since\n",
      "> graph built\n",
      "> graph exported\n",
      "\n"
     ]
    }
   ],
   "source": [
    "update_data()\n",
    "data_confirmed, data_deaths, countries = data_import()\n",
    "\n",
    "for log in False, True:\n",
    "    # Confirmed cases\n",
    "    name = \"cases\"\n",
    "    print(name)\n",
    "    chart(countries=countries,\n",
    "          data = data_confirmed, \n",
    "          data_rolling = data_confirmed, \n",
    "          by_million_inh = False, \n",
    "          last_d = round(len(data_confirmed)/2),\n",
    "          name_fig = name,\n",
    "          log=log\n",
    "         )\n",
    "\n",
    "    name = \"cases_per_1m_inhabitant\"\n",
    "    print(name)\n",
    "    chart(countries=countries,\n",
    "          data = data_confirmed, \n",
    "          data_rolling = data_confirmed, \n",
    "          by_million_inh = True, \n",
    "          last_d = round(len(data_confirmed)/2),\n",
    "          name_fig = name,\n",
    "          log=log\n",
    "         )\n",
    "\n",
    "    \"\"\"name = \"cases_per_1m_inhabitant_aligned\"\n",
    "    print(name)\n",
    "    chart(countries=countries,\n",
    "          data = data_confirmed, \n",
    "          data_rolling = data_confirmed, \n",
    "          by_million_inh = True, \n",
    "          last_d = 40, \n",
    "          align_curves = True,\n",
    "          offset_name = 'offset_confirmed',\n",
    "          name_fig = name,\n",
    "          log=log\n",
    "         )\"\"\"\n",
    "\n",
    "    name = \"cases_per_1m_inhabitant_since\"\n",
    "    print(name)\n",
    "    chart(countries=countries,\n",
    "          data = data_confirmed, \n",
    "          data_rolling = data_confirmed, \n",
    "          by_million_inh = True, \n",
    "          align_curves = False,\n",
    "          since=True,\n",
    "          name_fig = name,\n",
    "          min_rate=20,\n",
    "          log=log\n",
    "         )\n",
    "\n",
    "    \n",
    "    name = \"cases_since\"\n",
    "    print(name)\n",
    "    chart(countries=countries,\n",
    "          data = data_confirmed, \n",
    "          data_rolling = data_confirmed, \n",
    "          by_million_inh = False, \n",
    "          align_curves = False,\n",
    "          since=True,\n",
    "          name_fig = name,\n",
    "          min_rate=1000,\n",
    "          log=log\n",
    "         )\n",
    "    \n",
    "\n",
    "    # Deaths\n",
    "    name = \"deaths\"\n",
    "    print(name)\n",
    "    chart(countries=countries,\n",
    "          data = data_deaths, \n",
    "          data_rolling = data_deaths, \n",
    "          by_million_inh = False, \n",
    "          last_d = round(len(data_deaths)/2),\n",
    "          type_ppl = \"deaths\",\n",
    "          name_fig = name,\n",
    "          log=log\n",
    "         )\n",
    "    \n",
    "    \"\"\"name = \"deaths_new_since\"\n",
    "    print(name)\n",
    "    chart(countries = countries,\n",
    "          new = \"_new\",\n",
    "          data = data_deaths,\n",
    "          data_rolling = data_deaths, \n",
    "          by_million_inh = False, \n",
    "          last_d = round(len(data_deaths)/2),\n",
    "          type_ppl = \"deaths\",\n",
    "          name_fig = name,\n",
    "          since=True,\n",
    "          min_rate=10,\n",
    "          log=log\n",
    "         )\"\"\"\n",
    "\n",
    "    name = \"deaths_per_1m_inhabitant\"\n",
    "    print(name)\n",
    "    chart(countries=countries,\n",
    "          data = data_deaths, \n",
    "          data_rolling = data_deaths, \n",
    "          by_million_inh = True, \n",
    "          last_d = round(len(data_deaths)/2),\n",
    "          type_ppl = \"deaths\",\n",
    "          name_fig = name,\n",
    "          log=log\n",
    "         )\n",
    "\n",
    "    \"\"\"name = \"deaths_per_1m_inhabitant_aligned\"\n",
    "    print(name)\n",
    "    chart(countries=countries,\n",
    "          data = data_deaths, \n",
    "          data_rolling = data_deaths, \n",
    "          by_million_inh = True, \n",
    "          last_d = 35, \n",
    "          align_curves = True,\n",
    "          offset_name = 'offset_deaths',\n",
    "          type_ppl = \"deaths\",\n",
    "          name_fig = name,\n",
    "          log=log\n",
    "         )\"\"\"\n",
    "\n",
    "    name = \"deaths_per_1m_inhabitant_since\"\n",
    "    print(name)\n",
    "    chart(countries=countries,\n",
    "          data = data_deaths, \n",
    "          data_rolling = data_deaths, \n",
    "          by_million_inh = True, \n",
    "          align_curves = False,\n",
    "          type_ppl = \"deaths\",\n",
    "          since=True,\n",
    "          name_fig = name,\n",
    "          min_rate=3,\n",
    "          log=log\n",
    "         )\n",
    "    \n",
    "    name = \"deaths_since\"\n",
    "    print(name)\n",
    "    chart(countries=countries,\n",
    "          data = data_deaths, \n",
    "          data_rolling = data_deaths, \n",
    "          by_million_inh = False, \n",
    "          last_d = 20, \n",
    "          align_curves = False,\n",
    "          type_ppl = \"deaths\",\n",
    "          since=True,\n",
    "          name_fig = name,\n",
    "          min_rate=100,\n",
    "          log=log\n",
    "         )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTATIONS (SEIR model)\n",
    "Currently not working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "t_max = 100\n",
    "dt = .1\n",
    "t = np.linspace(0, t_max, int(t_max/dt) + 1)\n",
    "N = 10000\n",
    "init_vals = 1 - 1/N, 1/N, 0, 0\n",
    "alpha = 0.2\n",
    "beta = 1.75\n",
    "gamma = 0.5\n",
    "rho = 0.5\n",
    "params = alpha, beta, gamma, rho\n",
    "# Run simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seir_model_with_soc_dist(init_vals, params, t):\n",
    "    S_0, E_0, I_0, R_0 = init_vals\n",
    "    S, E, I, R = [S_0], [E_0], [I_0], [R_0]\n",
    "    alpha, beta, gamma, rho = params\n",
    "    dt = t[1] - t[0]\n",
    "    for _ in t[1:]:\n",
    "        next_S = S[-1] - (rho*beta*S[-1]*I[-1])*dt\n",
    "        next_E = E[-1] + (rho*beta*S[-1]*I[-1] - alpha*E[-1])*dt\n",
    "        next_I = I[-1] + (alpha*E[-1] - gamma*I[-1])*dt\n",
    "        next_R = R[-1] + (gamma*I[-1])*dt\n",
    "        S.append(next_S)\n",
    "        E.append(next_E)\n",
    "        I.append(next_I)\n",
    "        R.append(next_R)\n",
    "    return np.stack([S, E, I, R]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = seir_model_with_soc_dist(init_vals, params, t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
